<!DOCTYPE html>
<html>
    <head>
      <link rel="stylesheet" href="../../css/blather.css">

    </head>
    <body>
        <header><h1></h1></header>
        <main>

            <h2>Introduction</h2>

            <p>What does it mean when sometimes Beta distribution is referred to as "pseudo observations"?</p>

            <p>We'll be looking at a simple binomial model, y &sim; Binomial(&theta;) and &theta; &sim; Beta(&alpha;, &beta;).</p>

            <p>Let us say we have the following observations:</p>

            <table>
                <tr>
                    <td></td>
                </tr>
            </table>

            <p>What do we know about the unknown &theta;?</p>

            <h2>P(&theta;)</h2>

            <o>Let us begin by defining a prior distribution as a function. Now, in a real world setting you'd obviously use actual knowledge, but for simplicity I will be assuming that alla &theta; are equally probable. In other words, we'll use Uniform distribution with the parameters 0 and 1 as a prior. We can draw this in R: </o>

            <pre>
curve(dunif(x, 0, 1))                
            </pre>

            <p>What is important to notice is that value of the function is 1 between 0 and 1. If you are familiar with continuous probaility distributions, you know that their area should equal 1. In this case we have a square whose width and height are 1 so that checks out.</p>

            <p>But what is more important to us is that this means that we can effectively ignore the prior distribution. Since its value is 1 between the range we're interested in, it amounts to multiplying stuff with one, and that doesn't really change anything. </p>

            <p>Oh, but wait a minute! We were supposed to be using the <em>Beta</em> distribution as a prior! Well, Uniform(0, 1) = Beta(1, 1) so we're implicitly using Beta distribution as a prior -- I just thought it'd be simpler to talk about uniform distribution! Everything's in order, no worries!</p>

            <h2>P(y | &theta;)</h2>

            <p>Let us look at the likelihood function -- now we're getting to the meat. The probability of a y being 1 is &theta; and the probability of y being 0 is 1 - &theta;.  We can keep this functions separate, but for simplicity of exposition let us use x instead of &theta;. So the likelihood functions are</p>

            <p>x, if y = 1 and <br>1 - x, if y = 0</p>

            <p>in which x is constrained between 0 and 1.</p>

            <p>(x) (1 - x) = x &centerdot; 1 + x &centerdot; -x  = x - x<sup>2</sup></p>

            <p>What is probably the most mathematically challenging part follows, that is, one has to find the normalizing constant -- the area under the curve -- by integrating the function. Try it yourself first: it's not a particularly difficult integral! You can find integration rules e.g. from the site Math is Fun: ERROR LINK TO THE SITE. But any way, here's the solution: &int; x - x<sup>2</sup> = x<sup>2</sup> - x<sup>3</sup>/2</p>

            <p>Once we've found the indefinite integral we need to evaluate it with x = 0 and x = 1 and substract the first from the latter to find the area. We are in a good position since the integral evaluates to 0 when x = 0 (try it!) so we only need to evaluate it with x = 1. Let us do just that: 1<sup>2</sup> - 1<sup>3</sup>/2 = 1 - 1/3 = 2/3.</p>

            <p></p>

            <h2>Putting it all together</h2>

            <p>Alright, so if one begins from a position of complete agnosticism and updates their beliefs with the simple model used here, one arrives at a density function that's identical to a beta distribution. This means that we can also decompose a Beta distribution to individual likelihood functions.</p>

            <h2>The End</h2>

            <p>I hope this clarifies why the parameters for the Beta distribution are sometimes referred to as pseudo observations e.g. in the context of a Binomial model.</p>


        </main>
    </body>

</html>
