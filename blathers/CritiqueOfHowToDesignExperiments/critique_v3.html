<!DOCTYPE html>
<html>
  <head>
    <title>Critique</title>
    <style>
      main{
        margin-left: 5%;
        margin-right: 5%;
      }
      
      blockquote{
        font-style: italic;
        margin-left: 5%;
        margin-right: 5%;
      }
      
      h1{
        text-align: center;
        font-weight: 100;
        font-variant: small-caps;
        font-size: 2.5rem;
      }
      
      h2{
        font-weight: 100;
        font-variant: small-caps;
        font-size: 2rem;
      }
      
      p, li, blockquote{
        text-align: justify;
        font-size: 1.2rem;
      }
      
      footer{
        margin-top: 5%;
        margin-bottom: 5%;
        text-align: center;
      }

      .suppressMarker{
        list-style-type: none;
      }

      .normal{
        font-variant:normal;
      }
    </style>
  </head>
  <body>
  
    <h1>Misconceptions in the Book <i>How to Design and Report Experiments</i> by Field &amp; Hole</h1>
    
    <main>

      <p>My interest in the book mentioned in the title was sparked when I noticed it being used in the University of Jyväskylä, which is where I currently am enrolled in.  Unfortunately enough, I can't speak of the book kindly: it reproduces many common misconceptions about inferential statistics (some of which are so common that the have their own wikipedia page: <a href="https://en.wikipedia.org/wiki/Misuse_of_p-values" target="_blank">Misuse of <i>p</i>-values on Wikipedia</a>). But what's wrong with the book? Topics that I discuss are (not in order of importance):</p>

    <ol>
      <li><i>p</i>-values are defined incorrectly</li>
      <li><i>p</i>-value is taken as (kind of) a posterior probability</li>
      <li>Confidence intervals are defined incorrectly</li>
      <li>&chi;-squared test and independence of observations</li>
      <li>Statements about assumptions of parametric tests are confused</li>
    </ol>

    <p>To give this whine-a-thon some more context, I discuss these misconceptions from the viewpoint of statistics education and concrete scientific work in the conclusion. So stay tuned!</p>
    
    <h2><i><span class="normal">p</span></i>-values are defined incorrectly</h2>
    
    <p>The authors make the common erroneous interpretation that <i>p</i>-values&mdash;or statistical significance&mdash;inform us if the observed effect is due to "chance" or if it's "real". </p>

    <blockquote>&hellip; we calculate the probability that the results we have obtained are a chance result. (p. 142)</blockquote>

    <blockquote>Once you've calculated your test statistic, you calculate the probability of that test statistic occurring by chance &hellip; (p. 150)</blockquote>

    <blockquote> As a general rule, the results of the inferential tests determine whether or not you have an 'effect', and whether or not you can treat it as 'real' as opposed to merely a chance, freaky result. (p. 338)</blockquote>
      
    <p>This is completely wrong.</p>
    
    <p>First of all, the whole of frequentist theory is based on thinking about data as a random variable. Theoretically speaking, then, if you follow the frequentist paradigm, you should always be operating the assumption that the observed data was generated by random chance. So, in a way, the probability that your data was generated by chance is always 1.0! </p>

    <p>But of course&mdash;this is the second point&mdash;the writers want to communicate something about the experimental manipulation: since things tend to fluctuate randomly, they want to calculate the probability that the observed difference is due to the experimental manipulation and not random fluctuation. Sadly enough, <i>p</i>-values can not quantify this, and this interpretation is maybe even <i>more</i> wrong, but I'll talk about this more under the next topic.</p>

    <p>Third, from a philosophical viewpoint it's quite preposterous to claim that you can somehow quantify how "random" a single observed test statistic is. It is just a single number! If you think you can do that, calculate how random this number is and email your result to me: -1/12.</p>

    <h2><i><span class="normal">p</span></i>-value is taken as (kind of) a posterior probability</h2>

    <p>The writers have found Cohen's famous paper <i>Earth is Round (p &lt; 0.05)</i> (Cohen, 1994) and based on it they correctly instruct (p. 151) that P(H<sub>0</sub> | y) &NotEqual; P(y | H<sub>0</sub>), ie. that the probability of observing the data y given the null hypothesis (p-value) is <i>not</i> the same as the probability of the null hypothesis being true given the observed data. </p>

    <p>All fine there, yet they fail to realize that <i>p</i>-value is <i>not</i> the probability for the alternative hypothesis <i>either</i>. They state (p. 142): </p>

    <blockquote>Inferential statistics tell us whether the experimental hypothesis is likely to be true &hellip;  Of course, we can never be completely sure that either hypothesis is correct, and so we end up working with probabilities. Specifically we calculate the probability that the results we have obtained are a chance result &hellip; only when we are 95% certain that a result is genuine (i.e. not a chance finding) should we accept it as being true.</blockquote>

    <p>Probability of the experimental hypothesis in Bayesian terms  is P(H<sub>1</sub> | y), which is the posterior probability for that hypothesis, and that is not the same thing as a <i>p</i>-value (or 1-p) nor is it something frequentist statistics can be used to calculate: in frequentist statistics hypotheses are fixed, not random, and so it is not meaningful to assign probabilities to them. </p>

    <p>Also (I'm grasping at straws to try to understand what the writers mean) if the <i>p</i>-value is taken as a quasi-likelihood, P(D|H), we would still need prior probabilities (P(H)) to invert it to P(H|D)&mdash;which is what is done in Bayesian statistics.</p>
    
    <h2>Confidence intervals are defined incorrectly</h2> 
  
    <p>Authors' definition of confidence intervals is confused&mdash;to say the least! I present just one example here, but be assured that there are plenty more:</p>
    
    <blockquote>So, when you see a 95% confidence interval for a mean think of it like this: if we'd collected 100 samples, then 95 of these samples would have a mean within the boundaries of the confidence interval. (p. 135)</blockquote>
    
    <p>This is <i>not</i> how confidence intervals are usually constructed, the goal in constructing them is actually pretty much the opposite of what is being said in the quote. Confidence intervals are constructed in such a way that, given specific assumptions, the population mean will be included in 95% of confidence intervals when we continuously sample them from the population.</p>
    
    <p>(Confidence intervals don't have to be constructed only for the mean, but that's what the authors talk about and it's maybe the most common case, so for brevity I will only talk about that kind of intervals).</p>
    
    <p>Another mistake they is that they think that the width of the confidence interval somehow reflects the quality of data:</p>
    
    <blockquote>If the mean represents the data well, then the confidence interval of that mean should be small indicating that 95% of sample means would be very similar to the one obtained. If the sample mean is a bad representation of the population then the confidence interval will be very wide indicating that a different sample might produce a mean quite different to the one we have. (pp. 135-136)</blockquote>
    
    <p>The width of the confidence interval has absolutely nothing to do with how well the mean represents the data. Actually, I'm not even sure if what they say makes any sense at all. The mean is a summary statistic calculated from the data&mdash;and that's it. Of course, if the data is asymmetric, or has extreme outliers, we might say that the arithmetic mean isn't a great descriptor of it, but this has <i>nothing</i> to do with confidence intervals, let alone their widths.</p>
    
    <p>On the other hand they do also say that a wide CI means that the sample mean is bad representation of the <i>population</i> (in the previous example they were talking about data/sample). This statement is also false, but it might make more sense (?) given their other misconceptions about CI's. And by that I don't  mean that the statement is in any sense correct, but I at least can understand <i>why</i> they are making it in the first place.</p>
    
    <p>(Rest of my commentary on this point can be found under endnote 1 where I try to resolve the oddities).</p>
    
    <p>But speaking of the width of the confidence interval and quality of data: ironically enough, when looking at the frequentist properties of confidence intervals in the context of the t-test, tight intervals are <i>less</i> likely to contain the population mean in the long run! So maybe wider intervals are better? (This is discussed briefly in Morey et. al. (2016) in the context of <i>relevant subsets</i>, and they also provide some references for this this. But this is also fairly easy to observe by simulation: generate a lot of CI's, bin them and estimate the probability of each bin containing the population mean.).</p>
    
    <h2>&chi;-squared test and independence of observations</h2>
    
    <p>Well, this is not entirely about the &chi;-squared test, but this idea is presented in conjunction with it, and the authors seem to think that it's something specific to it, so here we are.</p>
    
    <p>The authors instruct the reader on conducting the &chi;-squared test and they have the following to say about independence of observations (p. 262):</p> 
    
    <blockquote>Observations must be independent: each subject must contribute to one and only one category. Failure to observe this rule renders the Chi-Squared test results completely invalid.</blockquote>

    <p>What's wrong with this? Well first of all, <i>all</i> common statistical methods assume that observations are independent&mdash;this has absolutely nothing to do with this specific statistical test.</p>
    
    <p>Second, the authors' claim that each subject must contribute to one and only one category is a bit confused. After all, it is very common to study singular subjects and their performance! There is nothing wrong with collecting data from one subject and&mdash;if you so wish&mdash;to use the &chi;-squared test to assess dependencies between the variables.</p>
    
    <p>The authors <i>are</i> correct in saying that this violates the assumption of independence, but the truth is that <i>all</i> assumptions are violated all the time, because statistical models are simplifications, models, and&mdash;as George Box famously said&mdash;all models are wrong. The reality of statistical modeling in psychology is that there are all kinds of dependencies in within-subjects designs: if we followed the authors' advice to the tee, we would <i>never</i> use within subjects designs!</p>

    <p>Instead of these warnings about <i>never</i> to use e.g. &chi;-squared test in within-subjects designs, they should have a candid discussion about how modeling is just that: modeling. </p>
    
    <h2>Statements about assumptions of parametric tests are confused</h2>
    
    <p>The authors discuss assumptions that parametric tests make when trying to differentiate them from non-parametric tests (p. 269):</p>
    
    <blockquote>
    [Parametric models] assume that your data are:
    <ol>
        <li>normally-distributed;</li>
        <li>measurements of a continuous variable, on an interval or ratio scale of measurement;</li>
      <li>the amount of variation amongst the scores in each condition or group is roughly comparable (the conditions or groups have equal variances).</li>
    </ol></blockquote>
    
    <p>These are of course the kind of assumptions that many common models make (with a caveat I'll discuss later) but it is incorrect to claim&mdash;as the authors seem to do&mdash;that the stated assumptions are shared by <i>all</i> parametric models. Even the authors <i>themselves</i> briefly mention <i>t</i>-test for unequal variances (p. 164 - 165) but they've seemingly forgotten by now!</p>
    
    <p>Two more counter-examples:</p>
    
    <p>1) Latent variable models are parametric models that model ordinal or categorical data. 2) Generalized linear models (GLM's) are parametric models in which other distributions besides normal are used for the data (e.g. logistic regression uses binomial distribution, poisson regression poisson distribution and so on). (Technically most of the latent variable models that are commonly used are GLM's but I wanted to mention them separately since that's how the assumptions of parametric testing were stated in the source text).</p>

    <p>I want to stress that these counter-examples are not some weird edge-cases but widely used models&mdash;after one dares to explore further than t-tests and ANOVAs they've been exposed to during their introductory statistics courses.</p>

    <p>Here's the caveat I mentioned in the beginning: the <i>t</i>-test, for example, doesn't make assumptions about the distribution of <i>data</i>! The assumption actually is that the <i>sample means</i> should be normally distributed. I think the confusion arises from the fact that usually researchers make inferences about that assumption by looking at the distribution of the observed data. Ironically enough <i>Bayesian</i> methods usually model the distribution of data directly&mdash;so again (as was the case with interpreting <i>p</i>-values as posterior probabilities) the authors seem to long for some nice straightforward-no-questions-asked Bayesian treatment!</p>

    <p>(By happenstance I came across similar misconceptions in the wild while writing this. Look at endnote 2 if you are interested).</p>

    <h2>In conclusion</h2>

    <p>The authors of the book get pretty much all of the central concepts wrong when it comes to frequentist statistical inference&mdash;and we are not talking about petty scholarly disagreements, or simplifications for making the concepts easier to learn for beginners.</p>

    <p>The way the authors present these concepts will most likely cause confusion. If a hard-working student supplements this book with one that contains the <i>correct</i> definitions, how are they to resolve the contradiction? In the worst case they conclude that they don't really understand statistics and they give up trying to learn the methods properly. After all, it is <i>impossible</i> for them to learn the methods from contradictory material!</p>

    <p>(The above example is not made up: that happened to me, even though I wouldn't call myself hard-working&mdash;that was just embellishment. Then again, maybe other people are more clever than me and are able to resolve the contradictions that arise).</p>

    <p>These are misconceptions that are alive and well among researchers and they continue to hinder scientific progress. The book seems to be selling the idea that frequentist statistical testing, and <i>p</i>-values in particular, can somehow quantify the probability of an effect being real. This is not the case. Research literature is full of examples of researchers buying into that idea, and that demonstrably has lead to some unfortunate fallout: Wikipedia lists this misconception about statistical testing as one of the causes of the so-called replication crisis: <a href="https://en.wikipedia.org/wiki/Replication_crisis#Problem_with_null_hypothesis_testing" target="_blank">replication crisis on Wikipedia</a>.</p>

    <p>Misconceptions about the foundations of frequentist testing likely lead to misapplying the methods. If a researcher doesn't understand how sampling intentions affect <i>p</i>-values, they might not see a problem in stopping the experiment early or breaking up their data into multiple publications (so-called salami publication). Undoubtedly these misconceptions were why Brian Wansink felt comfortable confessing to p-hacking (also known as <a href="https://en.wikipedia.org/wiki/Data_dredging" target="_blank"">data dredging (Wikipedia)</a>) on his blog&mdash;a confession which subsequently lead to many of his papers being retracted. Read more about this on Wikipedia: <a href="https://en.wikipedia.org/wiki/Brian_Wansink#Retractions_and_corrections" target="_blank">Brian Wansink on Wikipedia</a>.</p>

    <p>As the last example I point to you to the study referenced in endnote 2. The authors' lack of understanding about parametric statistical models renders much of their discussion regarding statistics as gobbledygook&mdash;which is not ideal for a research paper on methodology.</p>

    <p>The reason I present these rather serious examples is not just a scare tactic: I am anticipating a common counter-argument. Especially when it comes to the interpretation of confidence intervals, proponents of frequentist statistics often argue that the differing interpretations of confidence intervals under frequentist and Bayesian frameworks is mostly semantic and does not have ramifications regarding scientific work in practice. What I want to show with the examples is that this can be a slippery slope: inadequate understanding of the methods can and <i>will</i> lead to serious misconceptions about how these methods can be used and what kind of inferences can be reasonably made, and it is this confusion that can have significant impact on the quality of scientific work.</p>
    
    <h2>Endnotes</h2>

    <h3>Endnote 1</h3>

    <p>This discussion about "accuracy" of the mean might clue us in as to what the authors mean: </p>

    <blockquote>Not only do the variance and standard deviation tell us about the accuracy of the mean as a model of our data set, it also tells us something about the shape of the distribution of scores. If you think about what we've learnt about the mean we know that if the mean represents the data well, most of the scores will cluster close to the mean (and the standard deviation will be small relative to the mean) - as such, the distribution of scores will be quite thin. (p. 131)</blockquote>

    <p>In this context their point seems to be that if we want to replace the raw data set with a single number, the mean will be less wrong on average if there's less variance; or maybe that if the mean and mode of the distribution are far from each other, the mean doesn't match with what we would (in a colloquial sense) typically observe. Fair enough, and in itself an important lesson, but this idea can't be generalized to how confidence intervals are interpreted.</p>
    
    <p>There's also a danger for misinterpreting what the mean actually is: we have to remember that it is nothing but a summary statistic! In this sense it is perhaps a bit silly to talk about how "accurate" it is or isn't; it can only be that if we implicitly assign some specific task to it, such as describing the typical observation from a distribution. But I digress.</p>

    <h3>Endnote 2</h3>

    <p>In their paper Chyung et al (2018) compare categorical and continuous rating scales. They say the following about what kind of statistical analyses categorical vs. continuous rating scales permit: </p>

    <blockquote> You would analyze the ordinal type data obtained from bi- or unipolar Likert type discrete rating scales with non-parametric statistical procedures, although you may decide to use parametric statistical analyses on ordinal type data if you find the data to be approximately normally distributed. On the other hand, continuous rating scales typically generate interval data, much like the data you would find from using a ruler, which increases the possibility of the data being normally distributed and allows the use of a wider range of statistical procedures. </blockquote>

    <p>It is easy to see that in this quote many of the same kind of confusions (as in the book) are reproduced: the writers seem to be under the impression that the use of parametric tests is predicated on the data being normally distributed. In reality, this is not the case, as was already discussed in the text. It suffices to remind that there are a lot of statistical procedures for discrete data.</p>

    <p>This is not really the focus of this critique, but I found the writers' claim that continuous scales "increase the possibility of the data being normally distributed" curious. I think their insistence on the <i>data</i> being normally distributed stems from their misunderstanding that parametric tests require that. Using latent variable modeling discrete ratings can be modeled very fluently with  the normal distribution; on the other hand if the continuous scale is bounded (e.g. between 0 and 100) the distribution of observed ratings will almost surely deviate from normality at the extrema, and a normal distribution model might not be the best choice, even though the data is continuous.</p>

    <p>Now, I'm not claiming that the writers of this particular paper have gotten their incorrect idea about parametric tests from this particular book; my point is that these incorrect ideas are ubiquitous and they do bleed to actual research conducted conducted by professionals who write research papers which are published in peer-reviewed journals. If other researchers learn their methods from these papers, they become the next vessels for these misconceptions. It's a vicious cycle!</p>
    
    <h2>Sources</h2>

    <p>Chyung, S.Y., Swanson, I., Roberts, K. &amp; Hankinson, A. (2018), Evidence-Based Survey Design: The Use of Continuous Rating Scales in Surveys. <i>Perf. Improv.</i>, 57: 38-48. https://doi.org/10.1002/pfi.21763</p>

    <p>Cohen, J. (1994) The Earth Is Round (p &lt; 0.05). <i>American Psychologist</i>, 49, 997-1003.</p>

    <p>Field, A. &amp; Hole, G. (2003/2013). <i>How to Design and Report Experiments</i>. Sage, London.</p>

    <p>Morey R.D, Hoekstra R., Rouder J.N., Lee M.D. &amp; Wagenmakers E.J. (2016) The fallacy of placing confidence in confidence intervals. <i>Psychon Bull Rev.</i> Feb;23(1):103-23. doi: 10.3758/s13423-015-0947-8. PMID: 26450628; PMCID: PMC4742505.</p>

    </main>
    <footer>&copy; Joni Pääkkö</footer>
  </body>
</html>
